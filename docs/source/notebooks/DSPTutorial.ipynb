{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DSP Tutorial\n",
    "## So, You Want to Add a Signal Processing Routine\n",
    "\n",
    "This tutorial will teach you multiple ways to write signal processing routines, incorporate them into a data analysis, and run that data analysis.\n",
    "\n",
    "OUTLINE: we will write a set of processors to measure the current of our waveforms. The processors will be written to demonstrate a variety of patterns used when writing processors for use in dspeed. After this, we will show how to incorporate these into the signal processing framework of dspeed.\n",
    "\n",
    "Section I: Writing processors\n",
    "1) ufuncs as processors: BL mean and BL subtraction using numpy\n",
    "2) numba guvectorize to write ufuncs: pole-zero\n",
    "3) non-trivial numba: decrease size of output: derivative\n",
    "4) object mode for wrapping other processors: gaussian convolution\n",
    "5) factory method for processors that require initialization: triangle convolution\n",
    "\n",
    "Section II: Creating and running a processing chain\n",
    "1) Making a json file\n",
    "2) Running build_dsp (link to other DSP tutorial)\n",
    "3) Running WaveformBrowser (link to WB tutorial, show only simple example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Before we begin, some setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from lgdo import lh5\n",
    "from legendtestdata import LegendTestData\n",
    "\n",
    "# Get some sample waveforms from LEGEND test data\n",
    "ldata = LegendTestData()\n",
    "raw_file = ldata.get_path(\"lh5/LDQTA_r117_20200110T105115Z_cal_geds_raw.lh5\")\n",
    "tab = lh5.read(\"geds/raw\", raw_file)\n",
    "wfs = tab[\"waveform\"].values.nda.astype(\"float32\")\n",
    "t = tab[\"waveform\"].dt.nda.reshape((100, 1)) * np.arange(wfs.shape[1])\n",
    "baselines = tab[\"baseline\"].nda\n",
    "\n",
    "# Set up default plot style\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 4)\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part I: Processing data in bulk using ufuncs\n",
    "\n",
    "Universal functions, or [ufuncs](https://numpy.org/doc/stable/user/basics.ufuncs.html), operate on numpy ndarrays in an element-wise fashion. Ufuncs have several features that make them valuable for signal processing:\n",
    "1) [Vectorized](https://numpy.org/doc/stable/glossary.html#term-vectorization): ufuncs operate on full arrays, element-by-element. This is much faster (in python) than manually looping over the array, and may enable hardware optimizations to speed up code\n",
    "2) [Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html): ufuncs can automatically figure out the size and type of an output array based on the inputs to the function, making them flexible and easy to use\n",
    "3) In place operations: ufuncs can store their output in pre-allocated memory, reducing the need for allocating/de-allocating arrays, which is an expensive operation\n",
    "\n",
    "For example, we can subtract the baseline from each of our waveforms as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_blsub = np.zeros_like(wfs)\n",
    "np.subtract(wfs, baselines.reshape((len(baselines), 1)), out=wf_blsub);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Our original waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t[0:10].T, wfs[0:10].T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Our processed waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t[0:10].T, wf_blsub[0:10].T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Woohoo, it worked! And it applied to all of our waveforms (since it is **vectorized**)! Even though there is a single baseline value for each of our length 5592 waveforms, **broadcasting** caused the correct value to be subtracted from every waveform sample, producing a `(100, 5592)` to match the input. Also note that we predefined our output `wf_blsub` and operated on it **in place** rather than creating a new array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Creating a simple processor with `numba.guvectorize`\n",
    "\n",
    "While ufuncs are powerful and can be combined to do lots of things, sometimes we want to break beyond the confines of the existing ufuncs and write more complex processors. For this, we use [numba](https://numba.pydata.org/), which will convert your python code into C and just-in-time (JIT) compile it for improved performance. In particular, we will use [guvectorize](https://numba.readthedocs.io/en/stable/user/vectorize.html#the-guvectorize-decorator) to write [generalized ufuncs (gufuncs)](https://numpy.org/doc/stable/reference/c-api/generalized-ufuncs.html).\n",
    "\n",
    "These gufuncs have the same advantageous properties that we outlined above for ufuncs, namely that they are vectorized, allow for broadcasting, and can operate in place. However, gufuncs have additional flexibility since they can be broadcast over arrays rather than just elements. In other words, they are not constrained to perform the same operation on each element of an array, and can instead be programmed to perform the same operation over sub-arrays. This is controlled by specifying the **signature** of the gufunc.\n",
    "\n",
    "To illustrate how this works, we will write a guvectorized processor that pole-zero corrects our waveforms to remove the exponential decay on the tails:\n",
    "\n",
    "\\* Note, that there are recommended conventions for writing gufuncs to be included in dspeed, which are outlined at the end of this tutorial. For simplicity and illustrative purposes, we _ignore_ these for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import guvectorize\n",
    "\n",
    "\n",
    "@guvectorize([\"(float32[:], float32, float32[:])\"], \"(n),()->(n)\", nopython=True)\n",
    "def polezero(w_in: np.ndarray, tau: float, w_out: np.ndarray):\n",
    "    const = np.exp(-1 / tau)\n",
    "    w_out[0] = w_in[0]\n",
    "    for i in range(1, len(w_in), 1):\n",
    "        w_out[i] = w_out[i - 1] + w_in[i] - w_in[i - 1] * const"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Let's walk through what that does:\n",
    "\n",
    "- `@guvectorize` \"decorates\" the python function we define, causing it to produce a JIT-compiled function instead of a normal python function\n",
    "\n",
    "- ```[\"(float32[:], float32, float32[:])\"]``` defines the the data types of the input arguments. In this case, we take an array of floats, a scalar float, and output an array of floats. Note that if the output is a scalar, we must still use the array notation `[:]` to ensure that the value can be changed in-place. Also note that multiple sets of inputs may be specified. (This is very similar to the C convention of passing pointers to variables/arrays so that we can change them inside of a function. Just replace the pointer symbol `*` with the array `[:]`!)\n",
    "\n",
    "- ```\"(n),()->(n)\"``` defines the \"signature\" of the function, which specifies the sub-arrays that we define our operation over. In this case, we supply a vector of length `n`, a scalar, and output a vector of length `n`. When we input arrays into our function, these are the right-most dimension, and it is important that the length of this dimension matches (i.e. `w_in` and `w_out` must have the same length). The function will implicitly broadcast over any additional dimensions (meaning we can supply arrays with >1 dimension). Type signatures can get more complex than this by employing additional characters to describe dimensions (e.g. matrix multiplication would look like ```(m,n),(n,p)->(m,p)```).\n",
    "\n",
    "- ```nopython=True``` tells numba to use JIT-compilation to produce speedier code. Alternatively, we can supply ```forceobj=True``` to _not_ JIT-compile things and instead just vectorize python code. This can be useful for wrapping functions from other libraries, as we will see later.\n",
    "\n",
    "- ```def polezero(w_in: np.ndarray, tau: float, w_out: np.ndarray):```: the definition of our function. This must have the same number of arguments as our signature and data types. The last argument `w_out` is our output. Note that the function can be run out-of-place by not supplying an argument, which will cause it to allocate and return a new array. The type annotations are not necessary, but are recommended for readability.\n",
    "\n",
    "- ```for i in range(1, len(w_in), 1)```: In the definition of our function, we use a loop. This is notable, because in base-python we ordinarily want to avoid loops for high-performance code. In this case, because numba is JIT-compiling C-code, however, this loop will perform very well.\n",
    "\n",
    "Now we will try out our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_pzcorr = np.zeros_like(wfs)\n",
    "polezero(wf_blsub, 10000, out=wf_pzcorr)\n",
    "plt.plot(t[0:10].T, wf_pzcorr[0:10].T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Huzzah, the decay has been removed from the tails!\n",
    "\n",
    "### Now, we will present some other examples, that utilize more advanced features of numba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Advanced `guvectorize`: changing shape of output\n",
    "\n",
    "Sometimes, the output of a processor will differ in size from the inputs. For example, this occurs often when doing convolutions, derivatives, or changing the sampling period of a waveform. As an example, we will implement a derivative using finite-difference across `n_samp` points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@guvectorize([\"(float32[:], float32[:])\"], \"(n),(m)\", nopython=True)\n",
    "def derivative(w_in: np.ndarray, w_out: np.ndarray):\n",
    "    n_samp = len(w_in) - len(w_out)\n",
    "    for i_samp in range(len(w_out)):\n",
    "        w_out[i_samp] = w_in[i_samp + n_samp] - w_in[i_samp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Some important differences between this and our \"basic\" example:\n",
    "\n",
    "- `\"(n),(m)\"`: notice that we are not using `->` to denote that `w_out` is an output. That is because numba only let's us define new signature sizes for inputs, since it cannot deduce `m` from the other inputs. This means that we must be careful about specifying the length of our output waveform when we allocate it. This also means that this processor will _only_ work in-place!\n",
    "\n",
    "- `n_samp = len(w_in) - len(w_out)`: also notice that we did not feed `n_samp` as an input, but instead calculate it inside of the function.\n",
    "\n",
    "Now, let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set n_samp and use it to shape our output array\n",
    "n_samp = 5\n",
    "wf_deriv = np.zeros((wfs.shape[0], wfs.shape[1] - n_samp), \"float32\")\n",
    "\n",
    "derivative(wf_blsub, wf_deriv)\n",
    "plt.plot(t[0:10, n_samp // 2 : -n_samp // 2].T, wf_deriv[0:10].T)\n",
    "plt.xlim(42000, 48000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Yippee, now we have current pulses!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Advanced `guvectorize`: wrapping functions from other python libraries\n",
    "\n",
    "Often, someone will have already written a well-optimized function to apply whatever processor we want. In this case, rather than reimplementing the function in numba, we can simply write a wrapper for it. As we will soon see, it is often not necessary (or optimal) to wrap a function in this way, but we will write an example for demonstration purposes. We will wrap scipy's [gaussian_filter1D](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter1d.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "\n",
    "@guvectorize([\"(float32[:], float32, float32[:])\"], \"(n),()->(n)\", forceobj=True)\n",
    "def gauss_filter(w_in: np.ndarray, sigma: float, w_out: np.ndarray):\n",
    "    gaussian_filter1d(w_in, sigma, output=w_out, mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### The important difference between this and our \"basic\" example:\n",
    "\n",
    "```forceobj=True```: instead of using `nopython` mode, which causes numba to convert our function to C and compile it, `forceobj` mode executes the function in python, after wrapping it in the `gufunc` interface. This makes the function vectorized, and enables broadcasting; this interface also makes plugging the function into the dspeed framework very simple (see below). However, as noted above, it is not always necessary or desirable to do this; if a function is already vectorized, wrapping it can be redundant (this is actually the case for the above example). When using `forceobj` mode, we lose the advantages of `nopython`, such as faster loops.\n",
    "\n",
    "Now, let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_deriv_gauss = np.zeros_like(wf_deriv)\n",
    "gauss_filter(wf_deriv, 3, wf_deriv_gauss)\n",
    "plt.plot(t[0:10, n_samp // 2 : -n_samp // 2].T, wf_deriv_gauss[0:10].T)\n",
    "plt.xlim(42000, 48000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Boo-yah! Look how smooth that is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Advanced `guvectorize`: writing a function that can be initialized\n",
    "\n",
    "Sometimes, we need to be able to provide extra steps that are used to set up a processor. For example, we may want to read in values from a file, or compute a convolution kernel. Often, supplying these as arguments to the function is cumbersome, so instead we apply the \"factory\" technique.\n",
    "\n",
    "The \"factory\" technique entails writing a python function that performs this setup, compiles a numbified function, and then returns that function so that we can apply it to waveforms. In the following example, we will use this method to construct a triangular convolution kernel and apply it to our waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangle_filter(length: int):\n",
    "    # build triangular kernel\n",
    "    kernel = np.concatenate(\n",
    "        [\n",
    "            np.arange(1, length // 2 + 1, dtype=\"f\"),\n",
    "            np.arange((length + 1) // 2, 0, -1, dtype=\"f\"),\n",
    "        ]\n",
    "    )\n",
    "    kernel /= np.sum(kernel)  # normalize\n",
    "\n",
    "    @guvectorize([\"(float32[:], float32[:])\"], \"(n)->(n)\", forceobj=True, cache=False)\n",
    "    def returned_filter(w_in: np.ndarray, w_out: np.ndarray):\n",
    "        w_out[:] = np.convolve(w_in, kernel, mode=\"same\")\n",
    "\n",
    "    return returned_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### What's happening here?\n",
    "\n",
    "- When we call the outer function `triangle_filter`, is is building a convolution kernel, and then having numba generate a gufunc that will convolve the input with this kernel. The returned function has this kernel hard-coded into it (meaning the kernel is not an argument and cannot be changed without re-calling `triangle_filter` to generate a new gufunc).\n",
    "- `cache=False`: by setting `cache=True`, we can tell numba to store a copy of the function to disk. This is not something we want with factory functions, since we want to make copies that are different from one another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_deriv_tri = np.zeros_like(wf_deriv)\n",
    "tri_filter = triangle_filter(10)\n",
    "tri_filter(wf_deriv, wf_deriv_tri)\n",
    "\n",
    "plt.plot(t[0:10, n_samp // 2 : -n_samp // 2].T, wf_deriv_tri[0:10].T)\n",
    "plt.xlim(42000, 48000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Hot dog, those are some smooth current pulses!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Conventions for writing dspeed processors\n",
    "\n",
    "In order to produce consistently styled, readable code with well-defined behaviors for errors, we follow several conventions ([see documentation here](https://dspeed.readthedocs.io/en/stable/manuals/build_dsp.html#writing-custom-processors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspeed.errors import DSPFatal\n",
    "from dspeed.utils import numba_defaults_kwargs as nb_kwargs\n",
    "\n",
    "\n",
    "@guvectorize(\n",
    "    [\"void(float32[:], float32, float32[:])\", \"void(float64[:], float64, float64[:])\"],\n",
    "    \"(n),()->(n)\",\n",
    "    **nb_kwargs,\n",
    ")\n",
    "def pole_zero(w_in: np.ndarray, t_tau: float, w_out: np.ndarray) -> None:\n",
    "    \"\"\"Apply a pole-zero cancellation using the provided time\n",
    "    constant to the waveform.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_in\n",
    "        the input waveform.\n",
    "    t_tau\n",
    "        the time constant of the exponential to be deconvolved.\n",
    "    w_out\n",
    "        the pole-zero cancelled waveform.\n",
    "\n",
    "    JSON Configuration Example\n",
    "    --------------------------\n",
    "\n",
    "    .. code-block :: json\n",
    "\n",
    "        \"wf_pz\": {\n",
    "            \"function\": \"pole_zero\",\n",
    "            \"module\": \"dsp_tutorial\",\n",
    "            \"args\": [\"wf_bl\", \"400*us\", \"wf_pz\"],\n",
    "            \"unit\": \"ADC\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    if np.isnan(t_tau) or t_tau == 0:\n",
    "        raise DSPFatal(\"t_tau must be a non-zero number\")\n",
    "\n",
    "    w_out[:] = np.nan\n",
    "\n",
    "    if np.isnan(w_in).any():\n",
    "        return\n",
    "\n",
    "    const = np.exp(-1 / t_tau)\n",
    "    w_out[0] = w_in[0]\n",
    "    for i in range(1, len(w_in), 1):\n",
    "        w_out[i] = w_out[i - 1] + w_in[i] - w_in[i - 1] * const"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "- Type signatures: we tend to prefer floating point types, since these can be set to NaN (not-a-number, see below). Only use other types if you know this will not be an issue. We usually create 2 versions of the type signature, one for 32-bit and one for 64-bit\n",
    "- variable naming conventions: for waveforms use `w_[descriptor]`; for timepoints use `t_[descriptor]`; for amplitude values use `a_descriptor`\n",
    "- nb_kwargs: we encode many of the options for `guvectorize` in `nb_kwargs`. Default values are determined using the environment variables [described here](https://dspeed.readthedocs.io/en/stable/manuals/build_dsp.html#global-numba-options). For example, we set `nopython = True` using these defaults. To override defaults, do `nb_kwargs( arg=val )`\n",
    "- Docstring: use the [scikit-hep style](https://scikit-hep.org/developer/style). In addition, add an example JSON block to add the processor (see below).\n",
    "- NaNs: for undefined behavior and errors associated with individual waveforms, our preferred approach is to set outputs to `NaN`, or not-a-number, (and to propagate `NaN`s to future processor outputs). Typically, we begin by defaulting outputs to NaN, and apply NaN checks to inputs before we begin the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Finally, let's write a python module, [tutorial_procs](tutorial_procs.py), containing each of our numbafied functions, written in the dspeed style. A module file created in this way can be installed so that it can be used by dspeed's DSP framework, as we will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Part II: Using dspeed to combine multiple processors\n",
    "\n",
    "We've just learned how to create DSP transforms; now we will see how to combine these into a single data analysis using the dspeed [ProcessingChain](https://dspeed.readthedocs.io/en/stable/api/dspeed.html#dspeed.processing_chain.build_processing_chain). ProcessingChain is an object that reads data from LH5 files, applies a sequence of processors, and then outputs the results into a new LH5 file. There are several advantages to using ProcessingChain:\n",
    "\n",
    "- Manages memory and file I/O for you in an efficient way\n",
    "- Performs unit conversions for you, so that you can apply unit-less processors\n",
    "- Automatically\\* deduces the properties (i.e. shapes, dtypes, units) of variables\n",
    "- Set up using portable, easy-to-use JSON files\n",
    "\n",
    "The remainder of this tutorial will show you how to build a JSON file to setup a processing chain, how to use that to process an LH5 file, and how to interactively view each step of the analysis using the [WaveformBrowser](https://dspeed.readthedocs.io/en/stable/api/dspeed.vis.html#module-dspeed.vis.waveform_browser). We will be building a python dictionary, but it is very easy to translate this into a json file. Broadly speaking, the structure we use to define our processors is:\n",
    "\n",
    "```\n",
    "\"processors\": {\n",
    "    \"[name of parameter(s)]\": {\n",
    "        \"function\": \"[name of function]\",\n",
    "        \"module\": \"[python module containing function]\",\n",
    "        \"args\": [ \"names\", \"of\", \"args\"],\n",
    "        ... other optional args ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\\* Automation is never a perfect process; we will see how to override automated choices below\n",
    "\n",
    "Now, let's build a JSON config (ok, actually a python dict) for the analysis we did above! For another example of a standard Ge detector analysis, see the [Intro to DSP tutorial](https://dspeed.readthedocs.io/en/stable/notebooks/IntroToDSP.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"processors\": {\n",
    "        \"wf_blsub\": {\n",
    "            \"function\": \"subtract\",\n",
    "            \"module\": \"numpy\",\n",
    "            \"args\": [\"astype(waveform, 'float32')\", \"baseline\", \"wf_blsub\"],\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "This will call `numpy.subtract(waveform, baseline, out=wf_blsub)`. The ProcessingChain will find `waveform` and `baseline` in the input file, and create `wf_blsub` with the correct shape, data type and units automatically. The ProcessingChain is also capable of parsing certain operations (such as `astype` or `+-+/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"processors\"].update(\n",
    "    {\n",
    "        \"wf_pz\": {\n",
    "            \"function\": \"pole_zero\",\n",
    "            \"module\": \"tutorial_procs\",\n",
    "            \"args\": [\"wf_blsub\", \"625*us\", \"wf_pz\"],\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "This will call the `polezero` function we wrote above (in typical operation, it is recommended that you write your functions into your own module and import from there). Note that we are defining the PZ-time constant in units of us rather than number of samples! `wf_blsub` will be autmotically found from our previously defined processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"processors\"].update(\n",
    "    {\n",
    "        \"wf_deriv\": {\n",
    "            \"function\": \"derivative\",\n",
    "            \"module\": \"tutorial_procs\",\n",
    "            \"args\": [\"wf_pz\", \"wf_deriv(shape = len(wf_pz)-5, period=wf_pz.period)\"],\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "This will call the `derivative` function we wrote above. Note that in this case, we are overriding the automatic decision by ProcessingChain to make `wf_deriv` and `wf_pz` the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"processors\"].update(\n",
    "    {\n",
    "        \"wf_deriv_gauss\": {\n",
    "            \"function\": \"gauss_filter\",\n",
    "            \"module\": \"tutorial_procs\",\n",
    "            \"args\": [\"wf_deriv\", \"50*ns\", \"wf_deriv_gauss\"],\n",
    "        },\n",
    "        \"wf_deriv_tri\": {\n",
    "            \"function\": \"triangle_filter\",\n",
    "            \"module\": \"tutorial_procs\",\n",
    "            \"args\": [\"wf_deriv\", \"wf_deriv_tri\"],\n",
    "            \"init_args\": [\"160*ns/wf_deriv.period\"],\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Recall that we created `triangle_filter` using the \"factory\" method to initialize it during setup. ProcessingChain will read out the initialization parameters from \"init_args\".\n",
    "\n",
    "Finally, we will add a couple of processors to extract the maximum current amplitude from each filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"processors\"].update(\n",
    "    {\n",
    "        \"A_raw\": {\n",
    "            \"function\": \"amax\",\n",
    "            \"module\": \"numpy\",\n",
    "            \"args\": [\"wf_deriv\", 1, \"A_raw\"],\n",
    "            \"kwargs\": {\"signature\": \"(n),()->()\", \"types\": [\"fi->f\"]},\n",
    "        },\n",
    "        \"A_gauss\": {\n",
    "            \"function\": \"amax\",\n",
    "            \"module\": \"numpy\",\n",
    "            \"args\": [\"wf_deriv_gauss\", 1, \"A_gauss\"],\n",
    "            \"kwargs\": {\"signature\": \"(n),()->()\", \"types\": [\"fi->f\"]},\n",
    "        },\n",
    "        \"A_tri\": {\n",
    "            \"function\": \"amax\",\n",
    "            \"module\": \"numpy\",\n",
    "            \"args\": [\"wf_deriv_tri\", 1, \"A_tri\"],\n",
    "            \"kwargs\": {\"signature\": \"(n),()->()\", \"types\": [\"fi->f\"]},\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "To extract the current amplitudes, we are calling `np.amax`, which is _not_ formatted as a ufunc. We can still use it by specifying the shape and type signals, using the \"kwargs\" entry as above.\n",
    "\n",
    "The final step to defining our ProcessingChain is to tell it which variables to output to file. The ProcessingChain will ensure that it runs all of the processors necessary to build the output variables (and it will ignore any extraneous processors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"outputs\"] = [\"A_raw\", \"A_gauss\", \"A_tri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Executing our Processing Chain\n",
    "\n",
    "Once you have defined a JSON DSP configuration file, you can use it to run `build_dsp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspeed import build_dsp\n",
    "\n",
    "build_dsp(\n",
    "    f_raw=raw_file,\n",
    "    f_dsp=\"test_dsp.lh5\",\n",
    "    dsp_config=config,\n",
    "    lh5_tables=\"geds/raw\",\n",
    "    write_mode=\"o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "A command line tool exists for doing this. Equivalently to the above command, you can do:\n",
    "\n",
    "```dspeed build-dsp -o test_dsp.lh5 -g geds/raw -c [config_file].json -w [raw_file].lh5```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh5.show(\"test_dsp.lh5\")\n",
    "print()\n",
    "display(lh5.read_as(\"geds/dsp\", \"test_dsp.lh5\", \"pd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Using the waveform browser to look at transforms\n",
    "\n",
    "The [WaveformBrowser](https://dspeed.readthedocs.io/en/stable/api/dspeed.vis.html#dspeed.vis.waveform_browser.WaveformBrowser) ([tutorial here](https://dspeed.readthedocs.io/en/stable/notebooks/WaveformBrowser.html)) is a tool meant to enable you to draw waveforms from LH5 data files. This tool is designed to interact with a ProcessingChain, and can be used to view the effects of processors (and debug them as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspeed.vis.waveform_browser import WaveformBrowser\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "print()\n",
    "browser = WaveformBrowser(\n",
    "    raw_file,\n",
    "    \"geds/raw\",\n",
    "    dsp_config=config,\n",
    "    lines=[\"wf_deriv_tri\", \"wf_deriv_gauss\", \"A_tri\"],\n",
    "    styles=[\n",
    "        {\"ls\": [\":\"] * 10, \"color\": mcolors.TABLEAU_COLORS.values()},\n",
    "        {\"ls\": [\"--\"] * 10, \"color\": mcolors.TABLEAU_COLORS.values()},\n",
    "    ],\n",
    "    legend=[\"A_t={A_tri} A_g={A_gauss}\"],\n",
    "    x_lim=(\"40*us\", \"50*us\"),\n",
    ")\n",
    "\n",
    "browser.draw_next(5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
